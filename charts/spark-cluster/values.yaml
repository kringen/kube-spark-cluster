namespace: spark

master:
  name: spark-master
  replicas: 1
  container:
    name: spark-base
    imageRepository: kringen
    image: spark-base
    imageVersion: 3.8.1
    command: ["/usr/local/spark/bin/spark-class"]
    args: ["org.apache.spark.deploy.master.Master"]
    env:
    - name: SPARK_MASTER_HOST
      value: "spark-master"
    - name: SPARK_TMP_DIR
      value: "/srv/spark/tmp"
    - name: SPARK_PID_DIR
      value: "/srv/spark/pids"
    - name: SPARK_LOG_DIR
      value: "/srv/spark/logs"
    - name: SPARK_WORKER_DIR
      value: "/srv/spark/work"
    - name: SPARK_LOCAL_DIRS
      value: "/srv/spark/tmp"
    - name: SPARK_DIST_CLASSPATH
      value: "/usr/local/hadoop/etc/hadoop/*:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/usr/local/hadoop/share/hadoop/tools/lib/*"
    - name: HADOOP_HOME
      value: "/usr/local/hadoop"
    - name: HADOOP_CONF_DIR
      value: "/usr/local/hadoop/etc/hadoop"
    ports:
    - containerPort: 8080
      name: frontend
    - containerPort: 7077
      name: service
  service:
    ports:
    - port: 8080
      name: frontend
    - port: 7077
      name: service

worker:
  name: spark-worker
  replicas: 1
  container:
    name: spark-base
    imageRepository: kringen
    image: spark-base
    imageVersion: 3.8.1
    command: ["/usr/local/spark/bin/spark-class"]
    args: ["org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077"]
    env:
    - name: SPARK_WORKER_CORES
      value: "2"
    - name: SPARK_WORKER_MEMORY
      value: "1g"
    - name: SPARK_TMP_DIR
      value: "/srv/spark/tmp"
    - name: SPARK_PID_DIR
      value: "/srv/spark/pids"
    - name: SPARK_LOG_DIR
      value: "/srv/spark/logs"
    - name: SPARK_WORKER_DIR
      value: "/srv/spark/work"
    - name: SPARK_LOCAL_DIRS
      value: "/srv/spark/tmp"
    - name: SPARK_DIST_CLASSPATH
      value: "/usr/local/hadoop/etc/hadoop/*:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/usr/local/hadoop/share/hadoop/tools/lib/*"
    - name: HADOOP_HOME
      value: "/usr/local/hadoop"
    - name: HADOOP_CONF_DIR
      value: "/usr/local/hadoop/etc/hadoop"
    resources:
      requests:
        memory: "1Gi"
        cpu: "1"
      limits:
        memory: "2Gi"
        cpu: "3"
    ports:
    - containerPort: 8081
      name: frontend
  service:
    ports:
    - port: 8080
      name: frontend
    - port: 7077
      name: service

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

nodeSelector: {}

tolerations: []

affinity: {}
